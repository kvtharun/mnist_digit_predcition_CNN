{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten,BatchNormalization\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x_train (60000, 28, 28)\n",
      "Dimension of x_test (10000, 28, 28)\n",
      "Dimension of y_test (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Now we will se our dataset\n",
    "\n",
    "print(\"Dimension of x_train\",x_train.shape)\n",
    "print(\"Dimension of x_test\",x_test.shape)\n",
    "print(\"Dimension of y_test\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[9])\n",
    "plt.show()\n",
    "print(y_train[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "#as we have 10 classes (0-9) \n",
    "#we need to prdeict one out of 10 which has high probability\n",
    "epochs = 30\n",
    "img_rows = 28 \n",
    "img_cols = 28\n",
    "#as each image is 28 by 28 pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =x_train.astype(float)\n",
    "x_train =x_train/255\n",
    "x_test =x_test.astype(float)\n",
    "x_test =x_test/255\n",
    "#y_train =y_train/255\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes = 10, dtype = 'float32')\n",
    "y_test = to_categorical(y_test, num_classes = 10, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format in which image are feed to our model \n",
    "#there are two cases possible either we can have channel first than image dimension\n",
    "#or we can have dimension first than channel \n",
    "#we have one channel as it is a grey scale image therfore '1'\n",
    "if K.image_data_format() =='channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0],1,img_rows,img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0],1,img_rows,img_cols)\n",
    "    input_shape = (1,img_rows,img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0],img_rows,img_cols,1)\n",
    "    x_test = x_test.reshape(x_test.shape[0],img_rows,img_cols,1)\n",
    "    input_shape = (img_rows,img_cols,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BY DEFAULT STRIDE IS 1\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = input_shape))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))  # 10 units for 10 classes\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "38/38 [==============================] - 20s 477ms/step - loss: 1.4564 - accuracy: 0.4848 - val_loss: 2.2961 - val_accuracy: 0.0982\n",
      "Epoch 2/30\n",
      "38/38 [==============================] - 18s 463ms/step - loss: 1.0299 - accuracy: 0.6468 - val_loss: 2.2711 - val_accuracy: 0.1476\n",
      "Epoch 3/30\n",
      "38/38 [==============================] - 18s 481ms/step - loss: 0.8947 - accuracy: 0.6937 - val_loss: 2.2331 - val_accuracy: 0.2378\n",
      "Epoch 4/30\n",
      "38/38 [==============================] - 18s 486ms/step - loss: 0.8031 - accuracy: 0.7230 - val_loss: 2.2038 - val_accuracy: 0.2990\n",
      "Epoch 5/30\n",
      "38/38 [==============================] - 18s 485ms/step - loss: 0.7448 - accuracy: 0.7382 - val_loss: 2.1631 - val_accuracy: 0.2910\n",
      "Epoch 6/30\n",
      "38/38 [==============================] - 20s 531ms/step - loss: 0.6960 - accuracy: 0.7507 - val_loss: 2.1109 - val_accuracy: 0.2713\n",
      "Epoch 7/30\n",
      "38/38 [==============================] - 21s 563ms/step - loss: 0.6596 - accuracy: 0.7614 - val_loss: 1.9138 - val_accuracy: 0.4064\n",
      "Epoch 8/30\n",
      "38/38 [==============================] - 19s 491ms/step - loss: 0.6349 - accuracy: 0.7624 - val_loss: 1.7213 - val_accuracy: 0.4901\n",
      "Epoch 9/30\n",
      "38/38 [==============================] - 1564s 42s/step - loss: 0.6024 - accuracy: 0.7723 - val_loss: 1.2078 - val_accuracy: 0.7919\n",
      "Epoch 10/30\n",
      "38/38 [==============================] - 16s 433ms/step - loss: 0.5936 - accuracy: 0.7703 - val_loss: 0.8418 - val_accuracy: 0.9027\n",
      "Epoch 11/30\n",
      "38/38 [==============================] - 19s 496ms/step - loss: 0.5671 - accuracy: 0.7759 - val_loss: 0.8413 - val_accuracy: 0.8292\n",
      "Epoch 12/30\n",
      "38/38 [==============================] - 19s 489ms/step - loss: 0.5502 - accuracy: 0.7802 - val_loss: 0.5657 - val_accuracy: 0.8920\n",
      "Epoch 13/30\n",
      "38/38 [==============================] - 19s 494ms/step - loss: 0.5345 - accuracy: 0.7807 - val_loss: 0.1955 - val_accuracy: 0.9849\n",
      "Epoch 14/30\n",
      "38/38 [==============================] - 28s 740ms/step - loss: 0.5233 - accuracy: 0.7881 - val_loss: 0.1188 - val_accuracy: 0.9885\n",
      "Epoch 15/30\n",
      "38/38 [==============================] - 20s 527ms/step - loss: 0.5157 - accuracy: 0.7860 - val_loss: 0.0831 - val_accuracy: 0.9905\n",
      "Epoch 16/30\n",
      "38/38 [==============================] - 21s 562ms/step - loss: 0.5076 - accuracy: 0.7858 - val_loss: 0.0616 - val_accuracy: 0.9904\n",
      "Epoch 17/30\n",
      "38/38 [==============================] - 19s 492ms/step - loss: 0.4988 - accuracy: 0.7901 - val_loss: 0.0480 - val_accuracy: 0.9924\n",
      "Epoch 18/30\n",
      "38/38 [==============================] - 17s 458ms/step - loss: 0.4942 - accuracy: 0.7883 - val_loss: 0.0639 - val_accuracy: 0.9880\n",
      "Epoch 19/30\n",
      "38/38 [==============================] - 18s 465ms/step - loss: 0.4813 - accuracy: 0.7943 - val_loss: 0.0454 - val_accuracy: 0.9909\n",
      "Epoch 20/30\n",
      "38/38 [==============================] - 18s 484ms/step - loss: 0.4795 - accuracy: 0.7916 - val_loss: 0.0385 - val_accuracy: 0.9919\n",
      "Epoch 21/30\n",
      "38/38 [==============================] - 18s 482ms/step - loss: 0.4691 - accuracy: 0.7948 - val_loss: 0.0357 - val_accuracy: 0.9925\n",
      "Epoch 22/30\n",
      "38/38 [==============================] - 18s 470ms/step - loss: 0.4657 - accuracy: 0.7989 - val_loss: 0.0329 - val_accuracy: 0.9930\n",
      "Epoch 23/30\n",
      "38/38 [==============================] - 18s 476ms/step - loss: 0.4608 - accuracy: 0.7988 - val_loss: 0.0337 - val_accuracy: 0.9928\n",
      "Epoch 24/30\n",
      "38/38 [==============================] - 18s 482ms/step - loss: 0.4601 - accuracy: 0.7996 - val_loss: 0.0316 - val_accuracy: 0.9930\n",
      "Epoch 25/30\n",
      "38/38 [==============================] - 18s 480ms/step - loss: 0.4465 - accuracy: 0.8069 - val_loss: 0.0341 - val_accuracy: 0.9925\n",
      "Epoch 26/30\n",
      "38/38 [==============================] - 19s 507ms/step - loss: 0.4449 - accuracy: 0.8084 - val_loss: 0.0353 - val_accuracy: 0.9921\n",
      "Epoch 27/30\n",
      "38/38 [==============================] - 18s 474ms/step - loss: 0.4407 - accuracy: 0.8078 - val_loss: 0.0428 - val_accuracy: 0.9918\n",
      "Epoch 28/30\n",
      "38/38 [==============================] - 18s 468ms/step - loss: 0.4375 - accuracy: 0.8109 - val_loss: 0.0301 - val_accuracy: 0.9935\n",
      "Epoch 29/30\n",
      "38/38 [==============================] - 18s 467ms/step - loss: 0.4323 - accuracy: 0.8138 - val_loss: 0.0339 - val_accuracy: 0.9929\n",
      "Epoch 30/30\n",
      "38/38 [==============================] - 18s 471ms/step - loss: 0.4262 - accuracy: 0.8122 - val_loss: 0.0284 - val_accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x292a7d38a50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size= 1579, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 11, 11, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 32)          25632     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 6, 6, 32)          128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 4, 4, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 2, 2, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1, 1, 64)          102464    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1, 1, 10)          650       \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1, 1, 10)          40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 1, 10)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 194656 (760.38 KB)\n",
      "Trainable params: 194252 (758.80 KB)\n",
      "Non-trainable params: 404 (1.58 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 0.0284 - accuracy: 0.9946\n",
      "Score is : 0.028394781053066254\n",
      "Accuracy : 0.9945999979972839\n"
     ]
    }
   ],
   "source": [
    "score ,acc = model.evaluate(x_test,y_test)\n",
    "print(\"Score is :\",score)\n",
    "print(\"Accuracy :\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming you have a pre-trained Keras model\n",
    "# model = ...\n",
    "\n",
    "# Save the model architecture to a JSON file\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the model weights to an HDF5 file\n",
    "model.save_weights(\"model_weights.h5\")\n",
    "\n",
    "# Optionally, if you want to save the entire model (architecture + weights) to a single HDF5 file:\n",
    "# model.save(\"full_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
